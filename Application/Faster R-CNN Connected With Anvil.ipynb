{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Faster R-CNN model connected with Anvil Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.17.1\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['backbone.body.0.0.weight', 'backbone.body.0.1.weight', 'backbone.body.0.1.bias', 'backbone.body.0.1.running_mean', 'backbone.body.0.1.running_var', 'backbone.body.1.block.0.0.weight', 'backbone.body.1.block.0.1.weight', 'backbone.body.1.block.0.1.bias', 'backbone.body.1.block.0.1.running_mean', 'backbone.body.1.block.0.1.running_var', 'backbone.body.1.block.1.0.weight', 'backbone.body.1.block.1.1.weight', 'backbone.body.1.block.1.1.bias', 'backbone.body.1.block.1.1.running_mean', 'backbone.body.1.block.1.1.running_var', 'backbone.body.2.block.0.0.weight', 'backbone.body.2.block.0.1.weight', 'backbone.body.2.block.0.1.bias', 'backbone.body.2.block.0.1.running_mean', 'backbone.body.2.block.0.1.running_var', 'backbone.body.2.block.1.0.weight', 'backbone.body.2.block.1.1.weight', 'backbone.body.2.block.1.1.bias', 'backbone.body.2.block.1.1.running_mean', 'backbone.body.2.block.1.1.running_var', 'backbone.body.2.block.2.0.weight', 'backbone.body.2.block.2.1.weight', 'backbone.body.2.block.2.1.bias', 'backbone.body.2.block.2.1.running_mean', 'backbone.body.2.block.2.1.running_var', 'backbone.body.3.block.0.0.weight', 'backbone.body.3.block.0.1.weight', 'backbone.body.3.block.0.1.bias', 'backbone.body.3.block.0.1.running_mean', 'backbone.body.3.block.0.1.running_var', 'backbone.body.3.block.1.0.weight', 'backbone.body.3.block.1.1.weight', 'backbone.body.3.block.1.1.bias', 'backbone.body.3.block.1.1.running_mean', 'backbone.body.3.block.1.1.running_var', 'backbone.body.3.block.2.0.weight', 'backbone.body.3.block.2.1.weight', 'backbone.body.3.block.2.1.bias', 'backbone.body.3.block.2.1.running_mean', 'backbone.body.3.block.2.1.running_var', 'backbone.body.4.block.0.0.weight', 'backbone.body.4.block.0.1.weight', 'backbone.body.4.block.0.1.bias', 'backbone.body.4.block.0.1.running_mean', 'backbone.body.4.block.0.1.running_var', 'backbone.body.4.block.1.0.weight', 'backbone.body.4.block.1.1.weight', 'backbone.body.4.block.1.1.bias', 'backbone.body.4.block.1.1.running_mean', 'backbone.body.4.block.1.1.running_var', 'backbone.body.4.block.2.fc1.weight', 'backbone.body.4.block.2.fc1.bias', 'backbone.body.4.block.2.fc2.weight', 'backbone.body.4.block.2.fc2.bias', 'backbone.body.4.block.3.0.weight', 'backbone.body.4.block.3.1.weight', 'backbone.body.4.block.3.1.bias', 'backbone.body.4.block.3.1.running_mean', 'backbone.body.4.block.3.1.running_var', 'backbone.body.5.block.0.0.weight', 'backbone.body.5.block.0.1.weight', 'backbone.body.5.block.0.1.bias', 'backbone.body.5.block.0.1.running_mean', 'backbone.body.5.block.0.1.running_var', 'backbone.body.5.block.1.0.weight', 'backbone.body.5.block.1.1.weight', 'backbone.body.5.block.1.1.bias', 'backbone.body.5.block.1.1.running_mean', 'backbone.body.5.block.1.1.running_var', 'backbone.body.5.block.2.fc1.weight', 'backbone.body.5.block.2.fc1.bias', 'backbone.body.5.block.2.fc2.weight', 'backbone.body.5.block.2.fc2.bias', 'backbone.body.5.block.3.0.weight', 'backbone.body.5.block.3.1.weight', 'backbone.body.5.block.3.1.bias', 'backbone.body.5.block.3.1.running_mean', 'backbone.body.5.block.3.1.running_var', 'backbone.body.6.block.0.0.weight', 'backbone.body.6.block.0.1.weight', 'backbone.body.6.block.0.1.bias', 'backbone.body.6.block.0.1.running_mean', 'backbone.body.6.block.0.1.running_var', 'backbone.body.6.block.1.0.weight', 'backbone.body.6.block.1.1.weight', 'backbone.body.6.block.1.1.bias', 'backbone.body.6.block.1.1.running_mean', 'backbone.body.6.block.1.1.running_var', 'backbone.body.6.block.2.fc1.weight', 'backbone.body.6.block.2.fc1.bias', 'backbone.body.6.block.2.fc2.weight', 'backbone.body.6.block.2.fc2.bias', 'backbone.body.6.block.3.0.weight', 'backbone.body.6.block.3.1.weight', 'backbone.body.6.block.3.1.bias', 'backbone.body.6.block.3.1.running_mean', 'backbone.body.6.block.3.1.running_var', 'backbone.body.7.block.0.0.weight', 'backbone.body.7.block.0.1.weight', 'backbone.body.7.block.0.1.bias', 'backbone.body.7.block.0.1.running_mean', 'backbone.body.7.block.0.1.running_var', 'backbone.body.7.block.1.0.weight', 'backbone.body.7.block.1.1.weight', 'backbone.body.7.block.1.1.bias', 'backbone.body.7.block.1.1.running_mean', 'backbone.body.7.block.1.1.running_var', 'backbone.body.7.block.2.0.weight', 'backbone.body.7.block.2.1.weight', 'backbone.body.7.block.2.1.bias', 'backbone.body.7.block.2.1.running_mean', 'backbone.body.7.block.2.1.running_var', 'backbone.body.8.block.0.0.weight', 'backbone.body.8.block.0.1.weight', 'backbone.body.8.block.0.1.bias', 'backbone.body.8.block.0.1.running_mean', 'backbone.body.8.block.0.1.running_var', 'backbone.body.8.block.1.0.weight', 'backbone.body.8.block.1.1.weight', 'backbone.body.8.block.1.1.bias', 'backbone.body.8.block.1.1.running_mean', 'backbone.body.8.block.1.1.running_var', 'backbone.body.8.block.2.0.weight', 'backbone.body.8.block.2.1.weight', 'backbone.body.8.block.2.1.bias', 'backbone.body.8.block.2.1.running_mean', 'backbone.body.8.block.2.1.running_var', 'backbone.body.9.block.0.0.weight', 'backbone.body.9.block.0.1.weight', 'backbone.body.9.block.0.1.bias', 'backbone.body.9.block.0.1.running_mean', 'backbone.body.9.block.0.1.running_var', 'backbone.body.9.block.1.0.weight', 'backbone.body.9.block.1.1.weight', 'backbone.body.9.block.1.1.bias', 'backbone.body.9.block.1.1.running_mean', 'backbone.body.9.block.1.1.running_var', 'backbone.body.9.block.2.0.weight', 'backbone.body.9.block.2.1.weight', 'backbone.body.9.block.2.1.bias', 'backbone.body.9.block.2.1.running_mean', 'backbone.body.9.block.2.1.running_var', 'backbone.body.10.block.0.0.weight', 'backbone.body.10.block.0.1.weight', 'backbone.body.10.block.0.1.bias', 'backbone.body.10.block.0.1.running_mean', 'backbone.body.10.block.0.1.running_var', 'backbone.body.10.block.1.0.weight', 'backbone.body.10.block.1.1.weight', 'backbone.body.10.block.1.1.bias', 'backbone.body.10.block.1.1.running_mean', 'backbone.body.10.block.1.1.running_var', 'backbone.body.10.block.2.0.weight', 'backbone.body.10.block.2.1.weight', 'backbone.body.10.block.2.1.bias', 'backbone.body.10.block.2.1.running_mean', 'backbone.body.10.block.2.1.running_var', 'backbone.body.11.block.0.0.weight', 'backbone.body.11.block.0.1.weight', 'backbone.body.11.block.0.1.bias', 'backbone.body.11.block.0.1.running_mean', 'backbone.body.11.block.0.1.running_var', 'backbone.body.11.block.1.0.weight', 'backbone.body.11.block.1.1.weight', 'backbone.body.11.block.1.1.bias', 'backbone.body.11.block.1.1.running_mean', 'backbone.body.11.block.1.1.running_var', 'backbone.body.11.block.2.fc1.weight', 'backbone.body.11.block.2.fc1.bias', 'backbone.body.11.block.2.fc2.weight', 'backbone.body.11.block.2.fc2.bias', 'backbone.body.11.block.3.0.weight', 'backbone.body.11.block.3.1.weight', 'backbone.body.11.block.3.1.bias', 'backbone.body.11.block.3.1.running_mean', 'backbone.body.11.block.3.1.running_var', 'backbone.body.12.block.0.0.weight', 'backbone.body.12.block.0.1.weight', 'backbone.body.12.block.0.1.bias', 'backbone.body.12.block.0.1.running_mean', 'backbone.body.12.block.0.1.running_var', 'backbone.body.12.block.1.0.weight', 'backbone.body.12.block.1.1.weight', 'backbone.body.12.block.1.1.bias', 'backbone.body.12.block.1.1.running_mean', 'backbone.body.12.block.1.1.running_var', 'backbone.body.12.block.2.fc1.weight', 'backbone.body.12.block.2.fc1.bias', 'backbone.body.12.block.2.fc2.weight', 'backbone.body.12.block.2.fc2.bias', 'backbone.body.12.block.3.0.weight', 'backbone.body.12.block.3.1.weight', 'backbone.body.12.block.3.1.bias', 'backbone.body.12.block.3.1.running_mean', 'backbone.body.12.block.3.1.running_var', 'backbone.body.13.block.0.0.weight', 'backbone.body.13.block.0.1.weight', 'backbone.body.13.block.0.1.bias', 'backbone.body.13.block.0.1.running_mean', 'backbone.body.13.block.0.1.running_var', 'backbone.body.13.block.1.0.weight', 'backbone.body.13.block.1.1.weight', 'backbone.body.13.block.1.1.bias', 'backbone.body.13.block.1.1.running_mean', 'backbone.body.13.block.1.1.running_var', 'backbone.body.13.block.2.fc1.weight', 'backbone.body.13.block.2.fc1.bias', 'backbone.body.13.block.2.fc2.weight', 'backbone.body.13.block.2.fc2.bias', 'backbone.body.13.block.3.0.weight', 'backbone.body.13.block.3.1.weight', 'backbone.body.13.block.3.1.bias', 'backbone.body.13.block.3.1.running_mean', 'backbone.body.13.block.3.1.running_var', 'backbone.body.14.block.0.0.weight', 'backbone.body.14.block.0.1.weight', 'backbone.body.14.block.0.1.bias', 'backbone.body.14.block.0.1.running_mean', 'backbone.body.14.block.0.1.running_var', 'backbone.body.14.block.1.0.weight', 'backbone.body.14.block.1.1.weight', 'backbone.body.14.block.1.1.bias', 'backbone.body.14.block.1.1.running_mean', 'backbone.body.14.block.1.1.running_var', 'backbone.body.14.block.2.fc1.weight', 'backbone.body.14.block.2.fc1.bias', 'backbone.body.14.block.2.fc2.weight', 'backbone.body.14.block.2.fc2.bias', 'backbone.body.14.block.3.0.weight', 'backbone.body.14.block.3.1.weight', 'backbone.body.14.block.3.1.bias', 'backbone.body.14.block.3.1.running_mean', 'backbone.body.14.block.3.1.running_var', 'backbone.body.15.block.0.0.weight', 'backbone.body.15.block.0.1.weight', 'backbone.body.15.block.0.1.bias', 'backbone.body.15.block.0.1.running_mean', 'backbone.body.15.block.0.1.running_var', 'backbone.body.15.block.1.0.weight', 'backbone.body.15.block.1.1.weight', 'backbone.body.15.block.1.1.bias', 'backbone.body.15.block.1.1.running_mean', 'backbone.body.15.block.1.1.running_var', 'backbone.body.15.block.2.fc1.weight', 'backbone.body.15.block.2.fc1.bias', 'backbone.body.15.block.2.fc2.weight', 'backbone.body.15.block.2.fc2.bias', 'backbone.body.15.block.3.0.weight', 'backbone.body.15.block.3.1.weight', 'backbone.body.15.block.3.1.bias', 'backbone.body.15.block.3.1.running_mean', 'backbone.body.15.block.3.1.running_var', 'backbone.body.16.0.weight', 'backbone.body.16.1.weight', 'backbone.body.16.1.bias', 'backbone.body.16.1.running_mean', 'backbone.body.16.1.running_var', 'backbone.fpn.inner_blocks.0.0.weight', 'backbone.fpn.inner_blocks.0.0.bias', 'backbone.fpn.inner_blocks.1.0.weight', 'backbone.fpn.inner_blocks.1.0.bias', 'backbone.fpn.layer_blocks.0.0.weight', 'backbone.fpn.layer_blocks.0.0.bias', 'backbone.fpn.layer_blocks.1.0.weight', 'backbone.fpn.layer_blocks.1.0.bias', 'rpn.head.conv.0.0.weight', 'rpn.head.conv.0.0.bias', 'rpn.head.cls_logits.weight', 'rpn.head.cls_logits.bias', 'rpn.head.bbox_pred.weight', 'rpn.head.bbox_pred.bias', 'roi_heads.box_head.fc6.weight', 'roi_heads.box_head.fc6.bias', 'roi_heads.box_head.fc7.weight', 'roi_heads.box_head.fc7.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias'])\n"
          ]
        }
      ],
      "source": [
        "# Load the checkpoint file and print its keys\n",
        "checkpoint = torch.load('/Users/kimberleycollins/Desktop/Thesis/Thesis/Prototype/Application/model_final.pth', map_location=torch.device('cpu'))\n",
        "print(checkpoint.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.1\n",
            "0.17.1\n"
          ]
        }
      ],
      "source": [
        "# Check the current version\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "# If needed, install specific versions known to be compatible\n",
        "# !pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Import after installation\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n",
            "Predictions: [{'boxes': tensor([[160.4899, 643.6084, 179.0468, 662.2588],\n",
            "        [615.6562, 641.9152, 635.9840, 660.0482],\n",
            "        [615.6990, 298.9256, 630.3109, 314.4924]]), 'labels': tensor([3, 3, 3]), 'scores': tensor([0.2216, 0.1666, 0.1246])}]\n",
            "Scores: tensor([0.2216, 0.1666, 0.1246])\n",
            "Max index: 0\n",
            "0: 1388x690 1 Picual -Spain- , 336.3ms\n",
            "Speed: 336.3ms inference\n",
            "Result: {'olive_name': 'Picual -Spain-', 'timing_message': '0: 1388x690 1 Picual -Spain- , 336.3ms', 'speed_message': 'Speed: 336.3ms inference', 'dominant_color': '#dadada', 'color_name': 'gainsboro', 'olive_color_message': 'The olives are unripe, ideal for pickling or early harvest products. Not ideal for oil extraction yet.', 'dish_suggestion': 'Perfect for lighter fare such as martinis, Greek salads, or paired with cheese and wine. Green olives can also be used in chicken tagine, olive bread, or as a garnish for fish dishes.', 'preservation_tips': 'Black olives are often preserved in oil or dry-cured. Dry-curing involves covering the olives in salt to draw out moisture, resulting in a wrinkled, concentrated flavor. After curing, they can be rinsed and stored in olive oil with herbs or lemon zest for added flavor. Storing in oil helps keep them moist and flavorful.', 'health_tips': 'To keep your preserved olives healthy, ensure they are stored in a cool, dark place, away from direct sunlight. If preserved in liquid (brine, vinegar, or oil), make sure the olives are fully submerged to prevent mold and oxidation. Containers should be sealed tightly to keep out air and contaminants. For olives preserved in oil, use sterilized jars and high-quality olive oil to prevent the growth of bacteria, including botulism. Regularly check your olives for signs of spoilage, such as off-odors, discoloration, or mold. If preserved in brine, changing the brine every few months can help maintain freshness and flavor. Always use clean utensils to handle olives to prevent contamination. Consider refrigerating after opening to prolong their shelf life, especially for olives in liquid preservations.', 'pdf_report': BlobMedia[application/pdf,2620 bytes,name=olive_analysis_report.pdf]}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 272\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m anvil\u001b[38;5;241m.\u001b[39mmedia\u001b[38;5;241m.\u001b[39mfrom_file(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Keep the server running\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[43manvil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_forever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/anvil/server.py:437\u001b[0m, in \u001b[0;36mwait_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    435\u001b[0m _get_connection()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import anvil.server\n",
        "import anvil.media\n",
        "from ultralytics import YOLO  # Adjust this import based on your YOLO version\n",
        "import os\n",
        "import cv2  # Import OpenCV\n",
        "from sklearn.cluster import KMeans\n",
        "from PIL import Image, ImageOps, UnidentifiedImageError\n",
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "from torchvision.transforms import functional as F\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "import textwrap\n",
        "import webcolors\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "# If needed, install specific versions known to be compatible\n",
        "# !pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Import after installation\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Define your class names based on the model's classes\n",
        "CLASS_NAMES = ['Arbequina -Spain-', 'Bella di Cerignola -Italy-', 'Nocellara del Belice -Italy-', 'Picual -Spain-']\n",
        "\n",
        "# Load the Faster R-CNN model\n",
        "def load_model(n_classes):\n",
        "    model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, n_classes)\n",
        "    \n",
        "    checkpoint = torch.load('/Users/kimberleycollins/Desktop/Thesis/Thesis/Prototype/Application/model_final.pth', map_location='cpu')\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Call the load model function with the correct number of classes\n",
        "model = load_model(5)\n",
        "\n",
        "# Connect to Anvil server\n",
        "anvil.server.connect(\"server_PKZW2MEMHK7Q5OPOFHBZZSA3-VMYLTPSHUVY2Y7J2\")\n",
        "\n",
        "def find_dominant_color(image):\n",
        "    try:\n",
        "        pixels = image.reshape((-1, 3))\n",
        "        kmeans = KMeans(n_clusters=1, n_init=10)\n",
        "        kmeans.fit(pixels)\n",
        "        dominant_color = kmeans.cluster_centers_.astype(int)[0]\n",
        "        return '#%02x%02x%02x' % tuple(dominant_color)\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding dominant color: {e}\")\n",
        "        return None\n",
        "\n",
        "def check_olive_color(hex_color):\n",
        "    try:\n",
        "        rgb_color = tuple(int(hex_color.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
        "        luminance = 0.299 * rgb_color[0] + 0.587 * rgb_color[1] + 0.114 * rgb_color[2]\n",
        "        \n",
        "        color_ranges = {\n",
        "            \"unripe\": (120, 180, 70),\n",
        "            \"partially_ripe\": (90, 110, 50),\n",
        "            \"fully_ripe\": (60, 0, 70),\n",
        "            \"overripe\": (20, 20, 20)\n",
        "        }\n",
        "        \n",
        "        ripeness = \"unknown\"\n",
        "        if rgb_color >= color_ranges[\"unripe\"] and luminance > 150:\n",
        "            ripeness = \"unripe\"\n",
        "        elif rgb_color >= color_ranges[\"partially_ripe\"] and 100 <= luminance <= 150:\n",
        "            ripeness = \"partially ripe\"\n",
        "        elif rgb_color >= color_ranges[\"fully_ripe\"] and 50 <= luminance <= 100:\n",
        "            ripeness = \"fully ripe\"\n",
        "        elif rgb_color <= color_ranges[\"overripe\"] or luminance < 50:\n",
        "            ripeness = \"overripe\"\n",
        "        \n",
        "        ripeness_messages = {\n",
        "            \"unripe\": \"The olives are unripe, ideal for pickling or early harvest products. Not ideal for oil extraction yet.\",\n",
        "            \"partially ripe\": \"The olives are partially ripe and may have a more bitter taste, suitable for certain types of processing. Best for harvesting if aiming for early-harvest olive oil.\",\n",
        "            \"fully ripe\": \"The olives appear to be fully ripe and ready for most culinary uses or oil extraction. Ideal time for harvesting for high-quality olive oil.\",\n",
        "            \"overripe\": \"The olives are overripe; they might be too soft or mushy, suitable only for some types of oil or immediate consumption. Immediate harvesting is recommended to avoid spoilage.\",\n",
        "            \"unknown\": \"It's difficult to determine the ripeness of these olives based on the image provided.\"\n",
        "        }\n",
        "        \n",
        "        return ripeness_messages[ripeness]\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error checking olive color: {e}\")\n",
        "        return \"An error occurred while analyzing olive color.\"\n",
        "\n",
        "def suggest_dishes_for_olives(olive_color_message):\n",
        "    if 'Black olives' in olive_color_message:\n",
        "        return (\"Ideal for bold-flavored dishes like tapenade, puttanesca sauce, Mediterranean pizza, or as a rich topping for Greek salads. Black olives can also be used to enhance the flavor of roast lamb or as part of a cheese and charcuterie board.\")\n",
        "    else:\n",
        "        return (\"Perfect for lighter fare such as martinis, Greek salads, or paired with cheese and wine. Green olives can also be used in chicken tagine, olive bread, or as a garnish for fish dishes.\")\n",
        "\n",
        "def olive_preservation_tips(color):\n",
        "    if color == 'green':\n",
        "        tips = (\"Green olives can be preserved in brine, vinegar, or olive oil. \"\n",
        "                \"Brining is a traditional method that involves soaking the olives in a salt water solution for several weeks, \"\n",
        "                \"allowing them to ferment slightly and develop their flavor. \"\n",
        "                \"After brining, they can be seasoned with herbs and stored in olive oil or vinegar for enhanced taste.\")\n",
        "    else:\n",
        "        tips = (\"Black olives are often preserved in oil or dry-cured. \"\n",
        "                \"Dry-curing involves covering the olives in salt to draw out moisture, resulting in a wrinkled, concentrated flavor. \"\n",
        "                \"After curing, they can be rinsed and stored in olive oil with herbs or lemon zest for added flavor. \"\n",
        "                \"Storing in oil helps keep them moist and flavorful.\")\n",
        "    return tips\n",
        "\n",
        "def olive_health_tips():\n",
        "    tips = (\n",
        "        \"To keep your preserved olives healthy, ensure they are stored in a cool, dark place, away from direct sunlight. \"\n",
        "        \"If preserved in liquid (brine, vinegar, or oil), make sure the olives are fully submerged to prevent mold and oxidation. \"\n",
        "        \"Containers should be sealed tightly to keep out air and contaminants. \"\n",
        "        \"For olives preserved in oil, use sterilized jars and high-quality olive oil to prevent the growth of bacteria, including botulism. \"\n",
        "        \"Regularly check your olives for signs of spoilage, such as off-odors, discoloration, or mold. \"\n",
        "        \"If preserved in brine, changing the brine every few months can help maintain freshness and flavor. \"\n",
        "        \"Always use clean utensils to handle olives to prevent contamination. \"\n",
        "        \"Consider refrigerating after opening to prolong their shelf life, especially for olives in liquid preservations.\"\n",
        "    )\n",
        "    return tips\n",
        "\n",
        "def generate_pdf_report(filename, olive_name, olive_color, color_name, olive_color_message, preservation_tips, health_tips, dish_suggestion, current_date, current_time):\n",
        "    c = canvas.Canvas(filename, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    def draw_wrapped_text(c, text, x, y, max_width, line_height):\n",
        "        lines = textwrap.wrap(text, width=max_width)\n",
        "        for line in lines:\n",
        "            c.drawString(x, y, line)\n",
        "            y -= line_height  # Adjust this value for line spacing\n",
        "\n",
        "    max_width = 100  # Adjust this value to control text wrapping width\n",
        "    line_height = 12  # Adjust this value for line spacing\n",
        "\n",
        "    c.setFont(\"Helvetica\", 14)\n",
        "    c.drawString(30, height - 50, \"Olive Analysis Report\")\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(30, height - 80, f\"Olive Name: {olive_name}\")\n",
        "    c.drawString(30, height - 100, f\"Olive Color: {olive_color} ({color_name})\")\n",
        "    \n",
        "    y_position = height - 130\n",
        "    draw_wrapped_text(c, f\"Olive Color Message: {olive_color_message}\", 30, y_position, max_width, line_height)\n",
        "    y_position -= line_height * (len(textwrap.wrap(f\"Olive Color Message: {olive_color_message}\", width=max_width)) + 1)\n",
        "    \n",
        "    draw_wrapped_text(c, f\"Preservation Tips: {preservation_tips}\", 30, y_position, max_width, line_height)\n",
        "    y_position -= line_height * (len(textwrap.wrap(f\"Preservation Tips: {preservation_tips}\", width=max_width)) + 1)\n",
        "    \n",
        "    draw_wrapped_text(c, f\"Health Tips: {health_tips}\", 30, y_position, max_width, line_height)\n",
        "    y_position -= line_height * (len(textwrap.wrap(f\"Health Tips: {health_tips}\", width=max_width)) + 1)\n",
        "    \n",
        "    draw_wrapped_text(c, f\"Suggested Dish: {dish_suggestion}\", 30, y_position, max_width, line_height)\n",
        "\n",
        "    # Add the current date and time at the end of the report\n",
        "    y_position -= 2 * line_height\n",
        "    c.drawString(30, y_position, f\"Report Date: {current_date}\")\n",
        "    y_position -= line_height\n",
        "    c.drawString(30, y_position, f\"Report Time: {current_time}\")\n",
        "\n",
        "    c.save()\n",
        "\n",
        "def get_color_name(hex_color):\n",
        "    try:\n",
        "        rgb_color = webcolors.hex_to_rgb(hex_color)\n",
        "        color_name = closest_color(rgb_color)\n",
        "        return color_name\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting color name: {e}\")\n",
        "        return \"Unknown color\"\n",
        "\n",
        "def closest_color(requested_color):\n",
        "    min_colors = {}\n",
        "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
        "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
        "        rd = (r_c - requested_color.red) ** 2\n",
        "        gd = (g_c - requested_color.green) ** 2\n",
        "        bd = (b_c - requested_color.blue) ** 2\n",
        "        min_colors[(rd + gd + bd)] = name\n",
        "    return min_colors[min(min_colors.keys())]\n",
        "\n",
        "@anvil.server.callable\n",
        "def classify_image(media):\n",
        "    try:\n",
        "        with anvil.media.TempFile(media) as filename:\n",
        "            image = Image.open(filename)\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "            \n",
        "            # Prepare the image for the model\n",
        "            img_tensor = F.to_tensor(image)\n",
        "            img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "            start_time = time.time()  # Start time before model inference\n",
        "            with torch.no_grad():\n",
        "                predictions = model(img_tensor)\n",
        "            inference_time = time.time() - start_time  # Time taken for model to predict\n",
        "            \n",
        "            # Debugging prints to check the predictions\n",
        "            print(f\"Predictions: {predictions}\")\n",
        "\n",
        "            if len(predictions[0]['boxes']) == 0:\n",
        "                return {\"error\": \"No olives detected in the image.\"}\n",
        "            \n",
        "            scores = predictions[0]['scores']\n",
        "            print(f\"Scores: {scores}\")\n",
        "\n",
        "            max_index = scores.argmax().item()\n",
        "            print(f\"Max index: {max_index}\")\n",
        "\n",
        "            bbox = predictions[0]['boxes'][max_index].numpy().astype(int)\n",
        "            class_index = predictions[0]['labels'][max_index].item()\n",
        "            olive_name = CLASS_NAMES[class_index]\n",
        "\n",
        "            print(f\"0: {img_tensor.shape[3]}x{img_tensor.shape[2]} 1 {olive_name} , {inference_time * 1000:.1f}ms\")\n",
        "            print(f\"Speed: {inference_time * 1000:.1f}ms inference\")\n",
        "\n",
        "            np_image = np.array(image)\n",
        "            olive_portion = np_image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
        "            dominant_color = find_dominant_color(olive_portion)\n",
        "\n",
        "            if dominant_color is None:\n",
        "                return {\"error\": \"Failed to find a dominant color in the selected portion of the image.\"}\n",
        "\n",
        "            # Convert dominant color to RGB and get the closest color name\n",
        "            rgb_color = webcolors.hex_to_rgb(dominant_color)\n",
        "            color_name = closest_color(rgb_color)\n",
        "            olive_color_message = check_olive_color(dominant_color)\n",
        "            dish_suggestion = suggest_dishes_for_olives(olive_color_message)\n",
        "            color = 'green' if 'Green' in olive_color_message else 'black'\n",
        "            preservation_tips = olive_preservation_tips(color)\n",
        "            health_tips = olive_health_tips()\n",
        "\n",
        "            # Get current date and time\n",
        "            current_date = datetime.now().strftime('%A, %B %d, %Y')\n",
        "            current_time = datetime.now().strftime('%H:%M:%S')\n",
        "\n",
        "            # Generate PDF report\n",
        "            pdf_filename = \"/tmp/olive_analysis_report.pdf\"\n",
        "            generate_pdf_report(pdf_filename, olive_name, dominant_color, color_name, olive_color_message, preservation_tips, health_tips, dish_suggestion, current_date, current_time)\n",
        "            pdf_report = anvil.media.from_file(pdf_filename, \"application/pdf\")\n",
        "\n",
        "            result = {\n",
        "                \"olive_name\": olive_name,\n",
        "                \"timing_message\": f\"0: {img_tensor.shape[3]}x{img_tensor.shape[2]} 1 {olive_name} , {inference_time * 1000:.1f}ms\",\n",
        "                \"speed_message\": f\"Speed: {inference_time * 1000:.1f}ms inference\",\n",
        "                \"dominant_color\": dominant_color,\n",
        "                \"color_name\": color_name,\n",
        "                \"olive_color_message\": olive_color_message,\n",
        "                \"dish_suggestion\": dish_suggestion,\n",
        "                \"preservation_tips\": preservation_tips,\n",
        "                \"health_tips\": health_tips,\n",
        "                \"pdf_report\": pdf_report\n",
        "            }\n",
        "\n",
        "            print(f\"Result: {result}\")\n",
        "            return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying image: {e}\")\n",
        "        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
        "\n",
        "@anvil.server.callable\n",
        "def generate_report(olive_name, olive_color, color_name, olive_color_message, preservation_tips, health_tips, dish_suggestion, current_date, current_time):\n",
        "    filename = \"/tmp/olive_analysis_report.pdf\"\n",
        "    generate_pdf_report(filename, olive_name, olive_color, color_name, olive_color_message, preservation_tips, health_tips, dish_suggestion, current_date, current_time)\n",
        "    return anvil.media.from_file(filename, \"application/pdf\")\n",
        "\n",
        "# Keep the server running\n",
        "anvil.server.wait_forever()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
